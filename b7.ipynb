{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdb9f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR  best_params={'clf__C': 1}  best_precision=0.704917\n",
      "LinearSVC  best_params={'clf__C': 0.5}  best_precision=0.704731\n",
      "KNN  best_params={'clf__n_neighbors': 11, 'clf__weights': 'distance'}  best_precision=0.702371\n",
      "GaussianNB  best_params={'clf__var_smoothing': 1e-09}  best_precision=0.553584\n",
      "DecisionTree  best_params={'clf__max_depth': 5, 'clf__min_samples_leaf': 5}  best_precision=0.735469\n",
      "RandomForest  best_params={'clf__max_depth': 10, 'clf__n_estimators': 400}  best_precision=0.722124\n",
      "\n",
      "=== 5-fold CV (6 features) â€” sorted by Precision(mean) ===\n",
      "       Model                                                Params  Precision(mean)  Precision(std)\n",
      "DecisionTree     {'clf__max_depth': 5, 'clf__min_samples_leaf': 5}         0.735469        0.011357\n",
      "DecisionTree     {'clf__max_depth': 5, 'clf__min_samples_leaf': 1}         0.735467        0.010835\n",
      "DecisionTree    {'clf__max_depth': 5, 'clf__min_samples_leaf': 10}         0.734881        0.011737\n",
      "RandomForest      {'clf__max_depth': 10, 'clf__n_estimators': 400}         0.722124        0.006677\n",
      "RandomForest       {'clf__max_depth': 5, 'clf__n_estimators': 200}         0.721161        0.013642\n",
      "RandomForest      {'clf__max_depth': 10, 'clf__n_estimators': 200}         0.720194        0.008065\n",
      "RandomForest       {'clf__max_depth': 5, 'clf__n_estimators': 400}         0.720167        0.013005\n",
      "DecisionTree     {'clf__max_depth': 3, 'clf__min_samples_leaf': 5}         0.713788        0.012274\n",
      "DecisionTree    {'clf__max_depth': 3, 'clf__min_samples_leaf': 10}         0.713788        0.012274\n",
      "DecisionTree     {'clf__max_depth': 3, 'clf__min_samples_leaf': 1}         0.713788        0.012274\n",
      "DecisionTree   {'clf__max_depth': 10, 'clf__min_samples_leaf': 10}         0.712429        0.016275\n",
      "          LR                                         {'clf__C': 1}         0.704917        0.016223\n",
      "          LR                                         {'clf__C': 3}         0.704853        0.016271\n",
      "          LR                                         {'clf__C': 2}         0.704853        0.016271\n",
      "          LR                                       {'clf__C': 0.5}         0.704835        0.016328\n",
      "          LR                                         {'clf__C': 5}         0.704794        0.016162\n",
      "          LR                                       {'clf__C': 0.1}         0.704763        0.016692\n",
      "   LinearSVC                                       {'clf__C': 0.5}         0.704731        0.016872\n",
      "   LinearSVC                                         {'clf__C': 1}         0.704669        0.016886\n",
      "   LinearSVC                                         {'clf__C': 5}         0.704669        0.016886\n",
      "   LinearSVC                                         {'clf__C': 2}         0.704669        0.016886\n",
      "   LinearSVC                                         {'clf__C': 3}         0.704669        0.016886\n",
      "   LinearSVC                                       {'clf__C': 0.1}         0.704649        0.017148\n",
      "         KNN  {'clf__n_neighbors': 11, 'clf__weights': 'distance'}         0.702371        0.009299\n",
      "DecisionTree    {'clf__max_depth': 10, 'clf__min_samples_leaf': 5}         0.701945        0.019641\n",
      "         KNN   {'clf__n_neighbors': 11, 'clf__weights': 'uniform'}         0.701825        0.007870\n",
      "DecisionTree    {'clf__max_depth': 10, 'clf__min_samples_leaf': 1}         0.700900        0.015024\n",
      "RandomForest    {'clf__max_depth': None, 'clf__n_estimators': 200}         0.699279        0.014333\n",
      "         KNN    {'clf__n_neighbors': 7, 'clf__weights': 'uniform'}         0.697855        0.005878\n",
      "RandomForest    {'clf__max_depth': None, 'clf__n_estimators': 400}         0.697012        0.013837\n",
      "         KNN   {'clf__n_neighbors': 7, 'clf__weights': 'distance'}         0.695133        0.007145\n",
      "         KNN    {'clf__n_neighbors': 5, 'clf__weights': 'uniform'}         0.690984        0.012024\n",
      "         KNN   {'clf__n_neighbors': 5, 'clf__weights': 'distance'}         0.689193        0.008886\n",
      "         KNN    {'clf__n_neighbors': 3, 'clf__weights': 'uniform'}         0.684270        0.010558\n",
      "DecisionTree {'clf__max_depth': None, 'clf__min_samples_leaf': 10}         0.683791        0.019660\n",
      "         KNN   {'clf__n_neighbors': 3, 'clf__weights': 'distance'}         0.680474        0.011613\n",
      "DecisionTree  {'clf__max_depth': None, 'clf__min_samples_leaf': 5}         0.664684        0.010125\n",
      "DecisionTree  {'clf__max_depth': None, 'clf__min_samples_leaf': 1}         0.647621        0.010533\n",
      "  GaussianNB                         {'clf__var_smoothing': 1e-08}         0.553584        0.005569\n",
      "  GaussianNB                         {'clf__var_smoothing': 1e-09}         0.553584        0.005569\n",
      "  GaussianNB                         {'clf__var_smoothing': 1e-07}         0.553584        0.005569\n",
      "\n",
      "Selected winner: DecisionTree  with CV precision = 0.735469\n"
     ]
    }
   ],
   "source": [
    "# Q7\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re, pickle, numpy as np, pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# 0) Data (NLTK twitter_samples)\n",
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "pos_all = twitter_samples.strings('positive_tweets.json')\n",
    "neg_all = twitter_samples.strings('negative_tweets.json')\n",
    "X_all = np.array(pos_all + neg_all, dtype=object)\n",
    "y_all = np.array([1]*len(pos_all) + [0]*len(neg_all), dtype=int)\n",
    "df = pd.DataFrame({\"text\": X_all, \"y\": y_all}).drop_duplicates(subset=\"text\").reset_index(drop=True)\n",
    "X_all, y_all = df[\"text\"].values, df[\"y\"].values\n",
    "\n",
    "# 1) Leakage-safe cleaning/tokenization (drop emoticons & obvious sentiment hashtags)\n",
    "emo_pat = re.compile(r'[:;=8xX][\\-^oO\\']?[\\)\\](DdpP/\\(\\|\\\\]|<3|:-\\(|:-\\)|:\\)|:\\(|;\\)|:D|XD', re.UNICODE)\n",
    "hash_sent_pat = re.compile(r'#(happy|sad|love|hate|blessed|fail|awesome|terrible|good|bad)\\b', re.IGNORECASE)\n",
    "url_pat = re.compile(r'https?://\\S+|www\\.\\S+'); num_pat = re.compile(r'\\b\\d+\\b')\n",
    "ttok_clean = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\n",
    "def clean_tokenize(text: str):\n",
    "    text = url_pat.sub(\" URL \", text)\n",
    "    text = num_pat.sub(\" NUM \", text)\n",
    "    text = re.sub(r'\\brt\\b', ' ', text)\n",
    "    text = emo_pat.sub(\" \", text)        # remove emoticons\n",
    "    text = hash_sent_pat.sub(\" \", text)  # remove obvious sentiment hashtags\n",
    "    toks = ttok_clean.tokenize(text)\n",
    "    return [t for t in toks if any(ch.isalpha() for ch in t)]\n",
    "\n",
    "# 2) Feature A: 2 class-conditional frequency features with normalization (Q3)\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class PNFreqFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, normalize=True):\n",
    "        self.normalize = normalize\n",
    "    def fit(self, X, y=None):\n",
    "        pos_c, neg_c = Counter(), Counter()\n",
    "        for text, lab in zip(X, y):\n",
    "            toks = clean_tokenize(text if isinstance(text, str) else str(text))\n",
    "            (pos_c if lab==1 else neg_c).update(toks)\n",
    "        self.pos_freq_ = defaultdict(float, {k: float(v) for k,v in pos_c.items()})\n",
    "        self.neg_freq_ = defaultdict(float, {k: float(v) for k,v in neg_c.items()})\n",
    "        self.n_train_  = float(len(X))\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        rows = []\n",
    "        for text in X:\n",
    "            toks = clean_tokenize(text if isinstance(text, str) else str(text))\n",
    "            pos_sum = sum(self.pos_freq_[t] for t in toks)\n",
    "            neg_sum = sum(self.neg_freq_[t] for t in toks)\n",
    "            if self.normalize:\n",
    "                L = max(len(toks), 1)\n",
    "                N = self.n_train_ * L   # as specified in Q3\n",
    "                pos_sum /= N; neg_sum /= N\n",
    "            rows.append([pos_sum, neg_sum])\n",
    "        return csr_matrix(np.asarray(rows, dtype=float))\n",
    "\n",
    "# 3) Feature B: 4 stylistic/emphasis features (non-leaky)\n",
    "class TextExtraFeats(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.neg_words = set([\n",
    "            \"not\",\"no\",\"never\",\"none\",\"cannot\",\"can't\",\"dont\",\"don't\",\"won't\",\"wont\",\n",
    "            \"isn't\",\"aint\",\"ain't\",\"hasn't\",\"havent\",\"haven't\",\"wasn't\",\"weren't\",\"nor\",\n",
    "            \"n't\",\"shouldn't\",\"couldn't\",\"wouldn't\",\"doesn't\",\"didn't\",\"hadn't\"\n",
    "        ])\n",
    "        self.ttok_raw = TweetTokenizer(preserve_case=True, strip_handles=True, reduce_len=True)\n",
    "        self.elong_pat = re.compile(r'(.)\\1{2,}', re.UNICODE)\n",
    "    def _one(self, text: str):\n",
    "        text = text if isinstance(text, str) else str(text)\n",
    "        exclam = min(text.count('!'), 3) / 3.0\n",
    "        toks = self.ttok_raw.tokenize(text)\n",
    "        word_toks = [t for t in toks if any(ch.isalpha() for ch in t)]\n",
    "        n = max(len(word_toks), 1)\n",
    "        neg_ratio   = sum(1 for t in word_toks if t.lower() in self.neg_words) / n\n",
    "        elong_ratio = sum(1 for t in word_toks if self.elong_pat.search(t)) / n\n",
    "        caps_ratio  = sum(1 for t in word_toks if len(t)>=2 and t.isupper()) / n\n",
    "        return [neg_ratio, exclam, elong_ratio, caps_ratio]\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X):\n",
    "        feats = [self._one(x) for x in X]\n",
    "        return csr_matrix(np.asarray(feats, dtype=float))\n",
    "\n",
    "# 4) Combine to 6 features\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "feat_union = FeatureUnion([(\"pn2\", PNFreqFeatures(normalize=True)),\n",
    "                           (\"sty4\", TextExtraFeats())])\n",
    "\n",
    "# 5) Models + CV on precision\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "lin_scaler = StandardScaler(with_mean=False)\n",
    "densify    = FunctionTransformer(lambda X: X.toarray(), accept_sparse=True)\n",
    "\n",
    "pipelines = {\n",
    "    \"LR\":  Pipeline([(\"feat\", feat_union), (\"scaler\", lin_scaler),\n",
    "                     (\"clf\", LogisticRegression(max_iter=5000, solver=\"liblinear\", random_state=RANDOM_STATE))]),\n",
    "    \"LinearSVC\": Pipeline([(\"feat\", feat_union), (\"scaler\", lin_scaler),\n",
    "                           (\"clf\", LinearSVC(random_state=RANDOM_STATE))]),\n",
    "    \"KNN\": Pipeline([(\"feat\", feat_union), (\"to_dense\", densify),\n",
    "                     (\"scaler\", StandardScaler(with_mean=True)),\n",
    "                     (\"clf\", KNeighborsClassifier())]),\n",
    "    \"GaussianNB\": Pipeline([(\"feat\", feat_union), (\"to_dense\", densify),\n",
    "                            (\"scaler\", StandardScaler(with_mean=True)),\n",
    "                            (\"clf\", GaussianNB())]),\n",
    "    \"DecisionTree\": Pipeline([(\"feat\", feat_union), (\"to_dense\", densify),\n",
    "                              (\"clf\", DecisionTreeClassifier(random_state=RANDOM_STATE))]),\n",
    "    \"RandomForest\": Pipeline([(\"feat\", feat_union), (\"to_dense\", densify),\n",
    "                              (\"clf\", RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1))]),\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    \"LR\": {\"clf__C\": [0.1, 0.5, 1, 2, 3, 5]},\n",
    "    \"LinearSVC\": {\"clf__C\": [0.1, 0.5, 1, 2, 3, 5]},\n",
    "    \"KNN\": {\"clf__n_neighbors\": [3,5,7,11], \"clf__weights\": [\"uniform\",\"distance\"]},\n",
    "    \"GaussianNB\": {\"clf__var_smoothing\": [1e-9, 1e-8, 1e-7]},\n",
    "    \"DecisionTree\": {\"clf__max_depth\": [3,5,10,None], \"clf__min_samples_leaf\": [1,5,10]},\n",
    "    \"RandomForest\": {\"clf__n_estimators\": [200,400], \"clf__max_depth\": [None,5,10]},\n",
    "}\n",
    "\n",
    "def table_from_grid(name, grid):\n",
    "    res = grid.cv_results_\n",
    "    return pd.DataFrame({\n",
    "        \"Model\":   [name]*len(res[\"params\"]),\n",
    "        \"Params\":  res[\"params\"],\n",
    "        \"Precision(mean)\": res[\"mean_test_score\"],\n",
    "        \"Precision(std)\":  res[\"std_test_score\"],\n",
    "    })\n",
    "\n",
    "rows, best_name, best_model, best_prec = [], None, None, -1.0\n",
    "for name, pipe in pipelines.items():\n",
    "    grid = GridSearchCV(pipe, param_grids[name], scoring=\"precision\", cv=skf, n_jobs=-1)\n",
    "    grid.fit(X_all, y_all)\n",
    "    rows.append(table_from_grid(name, grid))\n",
    "    print(f\"{name}  best_params={grid.best_params_}  best_precision={grid.best_score_:.6f}\")\n",
    "    if grid.best_score_ > best_prec:\n",
    "        best_name, best_model, best_prec = name, grid.best_estimator_, grid.best_score_\n",
    "\n",
    "cv6 = pd.concat(rows, ignore_index=True).sort_values(\"Precision(mean)\", ascending=False).reset_index(drop=True)\n",
    "print(\"\\n=== 5-fold CV (6 features) â€” sorted by Precision(mean) ===\")\n",
    "print(cv6.to_string(index=False))\n",
    "print(f\"\\nSelected winner: {best_name}  with CV precision = {best_prec:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b9a3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples (with possible duplicates): 10000\n",
      "         LR  best_params={'clf__C': 1}  best_precision=0.722041\n",
      "  LinearSVC  best_params={'clf__C': 0.1}  best_precision=0.722127\n",
      "        KNN  best_params={'clf__n_neighbors': 11, 'clf__weights': 'uniform'}  best_precision=0.712981\n",
      " GaussianNB  best_params={'clf__var_smoothing': 1e-09}  best_precision=0.555486\n",
      "DecisionTree  best_params={'clf__max_depth': 5, 'clf__min_samples_leaf': 1}  best_precision=0.733317\n",
      "RandomForest  best_params={'clf__max_depth': 10, 'clf__n_estimators': 200}  best_precision=0.729796\n",
      "\n",
      "=== 5-fold CV (6 features, NO leakage-safe cleaning) â€” sorted by Precision(mean) ===\n",
      "       Model                                                Params  Precision(mean)  Precision(std)\n",
      "DecisionTree     {'clf__max_depth': 5, 'clf__min_samples_leaf': 1}         0.733317        0.023321\n",
      "DecisionTree     {'clf__max_depth': 5, 'clf__min_samples_leaf': 5}         0.732797        0.023536\n",
      "DecisionTree    {'clf__max_depth': 5, 'clf__min_samples_leaf': 10}         0.732766        0.023439\n",
      "RandomForest      {'clf__max_depth': 10, 'clf__n_estimators': 200}         0.729796        0.008608\n",
      "RandomForest      {'clf__max_depth': 10, 'clf__n_estimators': 400}         0.729260        0.008642\n",
      "   LinearSVC                                       {'clf__C': 0.1}         0.722127        0.015254\n",
      "   LinearSVC                                         {'clf__C': 1}         0.722069        0.015254\n",
      "   LinearSVC                                         {'clf__C': 2}         0.722069        0.015254\n",
      "   LinearSVC                                       {'clf__C': 0.5}         0.722069        0.015254\n",
      "   LinearSVC                                         {'clf__C': 5}         0.722069        0.015254\n",
      "   LinearSVC                                         {'clf__C': 3}         0.722069        0.015254\n",
      "          LR                                         {'clf__C': 1}         0.722041        0.014976\n",
      "          LR                                         {'clf__C': 2}         0.721983        0.015001\n",
      "          LR                                         {'clf__C': 3}         0.721983        0.015001\n",
      "          LR                                         {'clf__C': 5}         0.721983        0.015001\n",
      "          LR                                       {'clf__C': 0.5}         0.721952        0.014857\n",
      "          LR                                       {'clf__C': 0.1}         0.721725        0.014698\n",
      "RandomForest       {'clf__max_depth': 5, 'clf__n_estimators': 400}         0.719237        0.008820\n",
      "RandomForest       {'clf__max_depth': 5, 'clf__n_estimators': 200}         0.717972        0.007466\n",
      "DecisionTree    {'clf__max_depth': 3, 'clf__min_samples_leaf': 10}         0.717057        0.007687\n",
      "DecisionTree     {'clf__max_depth': 3, 'clf__min_samples_leaf': 1}         0.717057        0.007687\n",
      "DecisionTree     {'clf__max_depth': 3, 'clf__min_samples_leaf': 5}         0.717057        0.007687\n",
      "         KNN   {'clf__n_neighbors': 11, 'clf__weights': 'uniform'}         0.712981        0.007879\n",
      "         KNN  {'clf__n_neighbors': 11, 'clf__weights': 'distance'}         0.712433        0.005629\n",
      "DecisionTree   {'clf__max_depth': 10, 'clf__min_samples_leaf': 10}         0.710481        0.022727\n",
      "DecisionTree    {'clf__max_depth': 10, 'clf__min_samples_leaf': 5}         0.710174        0.017081\n",
      "DecisionTree    {'clf__max_depth': 10, 'clf__min_samples_leaf': 1}         0.707825        0.016390\n",
      "         KNN    {'clf__n_neighbors': 7, 'clf__weights': 'uniform'}         0.706615        0.007113\n",
      "         KNN   {'clf__n_neighbors': 7, 'clf__weights': 'distance'}         0.705597        0.005509\n",
      "RandomForest    {'clf__max_depth': None, 'clf__n_estimators': 200}         0.704107        0.010385\n",
      "RandomForest    {'clf__max_depth': None, 'clf__n_estimators': 400}         0.703508        0.010961\n",
      "DecisionTree {'clf__max_depth': None, 'clf__min_samples_leaf': 10}         0.699798        0.010871\n",
      "         KNN    {'clf__n_neighbors': 5, 'clf__weights': 'uniform'}         0.697269        0.009155\n",
      "         KNN   {'clf__n_neighbors': 5, 'clf__weights': 'distance'}         0.695990        0.005337\n",
      "         KNN    {'clf__n_neighbors': 3, 'clf__weights': 'uniform'}         0.690109        0.008857\n",
      "         KNN   {'clf__n_neighbors': 3, 'clf__weights': 'distance'}         0.687654        0.008703\n",
      "DecisionTree  {'clf__max_depth': None, 'clf__min_samples_leaf': 5}         0.673996        0.010905\n",
      "DecisionTree  {'clf__max_depth': None, 'clf__min_samples_leaf': 1}         0.650727        0.015074\n",
      "  GaussianNB                         {'clf__var_smoothing': 1e-08}         0.555486        0.005507\n",
      "  GaussianNB                         {'clf__var_smoothing': 1e-09}         0.555486        0.005507\n",
      "  GaussianNB                         {'clf__var_smoothing': 1e-07}         0.555486        0.005507\n",
      "\n",
      "Selected winner: DecisionTree  with CV precision = 0.733317\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function <lambda> at 0x000002619A910E00>: attribute lookup <lambda> on __main__ failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 186\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSelected winner: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  with CV precision = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_prec\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msa6_pipeline_raw.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 186\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved sa6_pipeline_raw.pkl (full pipeline)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mPicklingError\u001b[0m: Can't pickle <function <lambda> at 0x000002619A910E00>: attribute lookup <lambda> on __main__ failed"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Q7 â€” Six-feature pipeline WITHOUT leakage-safe cleaning\n",
    "# (emoticons & sentiment hashtags are kept as-is)\n",
    "# ============================================================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re, pickle, numpy as np, pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# 0) Data: NLTK twitter_samples (balanced)\n",
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "pos_all = twitter_samples.strings('positive_tweets.json')\n",
    "neg_all = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# Use the full dataset; DO NOT deduplicate (to stay \"non leakage-safe\")\n",
    "X_all = np.array(pos_all + neg_all, dtype=object)\n",
    "y_all = np.array([1]*len(pos_all) + [0]*len(neg_all), dtype=int)\n",
    "print(f\"Total samples (with possible duplicates): {len(X_all)}\")\n",
    "\n",
    "# 1) Raw tokenization (NO removal of emoticons or hashtags)\n",
    "#    We only normalize URLs and numbers to stable tokens; keep case lowered.\n",
    "url_pat = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "num_pat = re.compile(r'\\b\\d+\\b')\n",
    "ttok_raw = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\n",
    "def raw_tokenize(text: str):\n",
    "    text = text if isinstance(text, str) else str(text)\n",
    "    text = url_pat.sub(\" URL \", text)\n",
    "    text = num_pat.sub(\" NUM \", text)\n",
    "    # keep 'rt', keep emoticons, keep hashtags as-is\n",
    "    toks = ttok_raw.tokenize(text)\n",
    "    # keep alphabetic or mixed tokens (allow hashtags/emoticons)\n",
    "    return [t for t in toks if any(ch.isalnum() for ch in t)]\n",
    "\n",
    "# 2) Feature A: 2 class-conditional frequency features (POS/NEG)\n",
    "#    IMPORTANT: Estimated per CV training fold (fit) to avoid target leakage across folds.\n",
    "#    NO removal of emoticons/hashtags here; they are allowed to contribute.\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class PNFreqFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Produce 2 features per document:\n",
    "      - pos_freq_sum: sum of token counts from the positive class\n",
    "      - neg_freq_sum: sum of token counts from the negative class\n",
    "    Normalized by N = n_train * len(s) (as in your course Q3 variant).\n",
    "    \"\"\"\n",
    "    def __init__(self, normalize=True):\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        pos_c, neg_c = Counter(), Counter()\n",
    "        for text, lab in zip(X, y):\n",
    "            toks = raw_tokenize(text)\n",
    "            (pos_c if lab==1 else neg_c).update(toks)\n",
    "        self.pos_freq_ = defaultdict(float, {k: float(v) for k,v in pos_c.items()})\n",
    "        self.neg_freq_ = defaultdict(float, {k: float(v) for k,v in neg_c.items()})\n",
    "        self.n_train_  = float(len(X))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        rows = []\n",
    "        for text in X:\n",
    "            toks = raw_tokenize(text)\n",
    "            pos_sum = sum(self.pos_freq_[t] for t in toks)\n",
    "            neg_sum = sum(self.neg_freq_[t] for t in toks)\n",
    "            if self.normalize:\n",
    "                L = max(len(toks), 1)\n",
    "                N = self.n_train_ * L\n",
    "                pos_sum /= N; neg_sum /= N\n",
    "            rows.append([pos_sum, neg_sum])\n",
    "        return csr_matrix(np.asarray(rows, dtype=float))\n",
    "\n",
    "# 3) Feature B: 4 stylistic/emphasis features (computed on raw text)\n",
    "#    - negation_ratio\n",
    "#    - exclamation_intensity (cap 3, scale to [0,1])\n",
    "#    - elongation_ratio (repeated chars >=3, e.g., 'soooo')\n",
    "#    - allcaps_ratio (tokens ALL CAPS, len>=2) -- case-insensitive tokenizer above,\n",
    "#      so use a case-preserving one here.\n",
    "class TextExtraFeats(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.neg_words = set([\n",
    "            \"not\",\"no\",\"never\",\"none\",\"cannot\",\"can't\",\"dont\",\"don't\",\"won't\",\"wont\",\n",
    "            \"isn't\",\"aint\",\"ain't\",\"hasn't\",\"havent\",\"haven't\",\"wasn't\",\"weren't\",\"nor\",\n",
    "            \"n't\",\"shouldn't\",\"couldn't\",\"wouldn't\",\"doesn't\",\"didn't\",\"hadn't\"\n",
    "        ])\n",
    "        self.ttok_cap = TweetTokenizer(preserve_case=True, strip_handles=True, reduce_len=True)\n",
    "        self.elong_pat = re.compile(r'(.)\\1{2,}', re.UNICODE)\n",
    "\n",
    "    def _one(self, text: str):\n",
    "        text = text if isinstance(text, str) else str(text)\n",
    "        exclam = min(text.count('!'), 3) / 3.0\n",
    "        toks = self.ttok_cap.tokenize(text)\n",
    "        word_toks = [t for t in toks if any(ch.isalpha() for ch in t)]\n",
    "        n = max(len(word_toks), 1)\n",
    "        neg_ratio   = sum(1 for t in word_toks if t.lower() in self.neg_words) / n\n",
    "        elong_ratio = sum(1 for t in word_toks if self.elong_pat.search(t)) / n\n",
    "        caps_ratio  = sum(1 for t in word_toks if len(t)>=2 and t.isupper()) / n\n",
    "        return [neg_ratio, exclam, elong_ratio, caps_ratio]\n",
    "\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X):\n",
    "        feats = [self._one(x) for x in X]\n",
    "        return csr_matrix(np.asarray(feats, dtype=float))\n",
    "\n",
    "# 4) Combine to 6 features: (2 freq + 4 stylistic)\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "feat_union_6 = FeatureUnion([\n",
    "    (\"pn2\", PNFreqFeatures(normalize=True)),\n",
    "    (\"sty4\", TextExtraFeats())\n",
    "])\n",
    "\n",
    "# 5) Models + 5-fold CV (precision)\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# For linear models, we can scale sparse safely (with_mean=False).\n",
    "# For dense models (KNN/NB/Tree/RF), densify and optionally scale.\n",
    "lin_scaler = StandardScaler(with_mean=False)\n",
    "densify    = FunctionTransformer(lambda X: X.toarray(), accept_sparse=True)\n",
    "\n",
    "pipelines = {\n",
    "    \"LR\":  Pipeline([(\"feat\", feat_union_6), (\"scaler\", lin_scaler),\n",
    "                     (\"clf\", LogisticRegression(max_iter=5000, solver=\"liblinear\", random_state=RANDOM_STATE))]),\n",
    "    \"LinearSVC\": Pipeline([(\"feat\", feat_union_6), (\"scaler\", lin_scaler),\n",
    "                           (\"clf\", LinearSVC(random_state=RANDOM_STATE))]),\n",
    "    \"KNN\": Pipeline([(\"feat\", feat_union_6), (\"to_dense\", densify),\n",
    "                     (\"scaler\", StandardScaler(with_mean=True)),\n",
    "                     (\"clf\", KNeighborsClassifier())]),\n",
    "    \"GaussianNB\": Pipeline([(\"feat\", feat_union_6), (\"to_dense\", densify),\n",
    "                            (\"scaler\", StandardScaler(with_mean=True)),\n",
    "                            (\"clf\", GaussianNB())]),\n",
    "    \"DecisionTree\": Pipeline([(\"feat\", feat_union_6), (\"to_dense\", densify),\n",
    "                              (\"clf\", DecisionTreeClassifier(random_state=RANDOM_STATE))]),\n",
    "    \"RandomForest\": Pipeline([(\"feat\", feat_union_6), (\"to_dense\", densify),\n",
    "                              (\"clf\", RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1))]),\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    \"LR\": {\"clf__C\": [0.1, 0.5, 1, 2, 3, 5]},\n",
    "    \"LinearSVC\": {\"clf__C\": [0.1, 0.5, 1, 2, 3, 5]},\n",
    "    \"KNN\": {\"clf__n_neighbors\": [3,5,7,11], \"clf__weights\": [\"uniform\",\"distance\"]},\n",
    "    \"GaussianNB\": {\"clf__var_smoothing\": [1e-9, 1e-8, 1e-7]},\n",
    "    \"DecisionTree\": {\"clf__max_depth\": [3,5,10,None], \"clf__min_samples_leaf\": [1,5,10]},\n",
    "    \"RandomForest\": {\"clf__n_estimators\": [200,400], \"clf__max_depth\": [None,5,10]},\n",
    "}\n",
    "\n",
    "def cv_table(name, grid):\n",
    "    res = grid.cv_results_\n",
    "    return pd.DataFrame({\n",
    "        \"Model\":   [name]*len(res[\"params\"]),\n",
    "        \"Params\":  res[\"params\"],\n",
    "        \"Precision(mean)\": res[\"mean_test_score\"],\n",
    "        \"Precision(std)\":  res[\"std_test_score\"],\n",
    "    })\n",
    "\n",
    "rows, best_name, best_model, best_prec = [], None, None, -1.0\n",
    "for name, pipe in pipelines.items():\n",
    "    grid = GridSearchCV(pipe, param_grids[name], scoring=\"precision\", cv=skf, n_jobs=-1)\n",
    "    grid.fit(X_all, y_all)\n",
    "    rows.append(cv_table(name, grid))\n",
    "    print(f\"{name:>11s}  best_params={grid.best_params_}  best_precision={grid.best_score_:.6f}\")\n",
    "    if grid.best_score_ > best_prec:\n",
    "        best_name, best_model, best_prec = name, grid.best_estimator_, grid.best_score_\n",
    "\n",
    "cv6_raw = pd.concat(rows, ignore_index=True).sort_values(\"Precision(mean)\", ascending=False).reset_index(drop=True)\n",
    "print(\"\\n=== 5-fold CV (6 features, NO leakage-safe cleaning) â€” sorted by Precision(mean) ===\")\n",
    "print(cv6_raw.to_string(index=False))\n",
    "print(f\"\\nSelected winner: {best_name}  with CV precision = {best_prec:.6f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
